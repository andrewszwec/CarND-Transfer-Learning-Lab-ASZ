{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import scale\n",
    "import keras.regularizers as regularizers\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# command line flags\n",
    "flags.DEFINE_string('training_file', './inception-100/inception_cifar10_100_bottleneck_features_train.p', \"Bottleneck features training file (.p)\")\n",
    "flags.DEFINE_string('validation_file', './inception-100/inception_cifar10_bottleneck_features_validation.p', \"Bottleneck features validation file (.p)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_bottleneck_data(training_file, validation_file):\n",
    "    \"\"\"\n",
    "    Utility function to load bottleneck features.\n",
    "\n",
    "    Arguments:\n",
    "        training_file - String\n",
    "        validation_file - String\n",
    "    \"\"\"\n",
    "    print(\"Training file\", training_file)\n",
    "    print(\"Validation file\", validation_file)\n",
    "\n",
    "    with open(training_file, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    with open(validation_file, 'rb') as f:\n",
    "        validation_data = pickle.load(f)\n",
    "\n",
    "    X_train = train_data['features']\n",
    "    y_train = train_data['labels']\n",
    "    X_val = validation_data['features']\n",
    "    y_val = validation_data['labels']\n",
    "\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ./inception-100/inception_cifar10_100_bottleneck_features_train.p\n",
      "Validation file ./inception-100/inception_cifar10_bottleneck_features_validation.p\n",
      "(1000, 1, 1, 2048) (1000, 1)\n",
      "(10000, 1, 1, 2048) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# load bottleneck data\n",
    "X_train, y_train, X_val, y_val = load_bottleneck_data(FLAGS.training_file, FLAGS.validation_file)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape (1000, 10)\n",
      "y_val.shape (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess\n",
    "# X_train = scale(X_train, axis=1)\n",
    "# X_val = scale(X_train, axis=1)\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# One-hot encode target variable\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "\n",
    "print('y_train.shape', y_train.shape)\n",
    "print('y_val.shape', y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottleneck Shape (1, 1, 2048)\n",
      "Train on 1000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s - loss: 2.6272 - acc: 0.0880 - val_loss: 2.2855 - val_acc: 0.1333\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.5058 - acc: 0.1130 - val_loss: 2.2475 - val_acc: 0.2056\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.4423 - acc: 0.1250 - val_loss: 2.2186 - val_acc: 0.2667\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.3808 - acc: 0.1360 - val_loss: 2.1955 - val_acc: 0.3064\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.3491 - acc: 0.1480 - val_loss: 2.1753 - val_acc: 0.3326\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.2592 - acc: 0.1700 - val_loss: 2.1561 - val_acc: 0.3539\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.2444 - acc: 0.1910 - val_loss: 2.1372 - val_acc: 0.3745\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.2116 - acc: 0.2000 - val_loss: 2.1178 - val_acc: 0.3933\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.1759 - acc: 0.2060 - val_loss: 2.0968 - val_acc: 0.4089\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.1795 - acc: 0.2230 - val_loss: 2.0740 - val_acc: 0.4240\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.1529 - acc: 0.2030 - val_loss: 2.0506 - val_acc: 0.4369\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.1136 - acc: 0.2470 - val_loss: 2.0256 - val_acc: 0.4472\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.0743 - acc: 0.2400 - val_loss: 1.9985 - val_acc: 0.4565\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 0s - loss: 2.0470 - acc: 0.2830 - val_loss: 1.9687 - val_acc: 0.4684\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.9854 - acc: 0.2830 - val_loss: 1.9359 - val_acc: 0.4778\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.9772 - acc: 0.2880 - val_loss: 1.9008 - val_acc: 0.4871\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.9481 - acc: 0.3420 - val_loss: 1.8639 - val_acc: 0.4952\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.9032 - acc: 0.3440 - val_loss: 1.8252 - val_acc: 0.5031\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.9136 - acc: 0.3300 - val_loss: 1.7862 - val_acc: 0.5107\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.8444 - acc: 0.3510 - val_loss: 1.7469 - val_acc: 0.5164\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.7885 - acc: 0.4020 - val_loss: 1.7065 - val_acc: 0.5224\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.7532 - acc: 0.4220 - val_loss: 1.6651 - val_acc: 0.5287\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.7338 - acc: 0.4070 - val_loss: 1.6236 - val_acc: 0.5341\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.7216 - acc: 0.3970 - val_loss: 1.5832 - val_acc: 0.5376\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.6813 - acc: 0.4360 - val_loss: 1.5442 - val_acc: 0.5425\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.6534 - acc: 0.4330 - val_loss: 1.5078 - val_acc: 0.5458\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.6150 - acc: 0.4410 - val_loss: 1.4730 - val_acc: 0.5516\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.5960 - acc: 0.4390 - val_loss: 1.4401 - val_acc: 0.5548\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.5324 - acc: 0.4830 - val_loss: 1.4096 - val_acc: 0.5581\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.5151 - acc: 0.4840 - val_loss: 1.3810 - val_acc: 0.5635\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.4593 - acc: 0.5040 - val_loss: 1.3551 - val_acc: 0.5668\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.4461 - acc: 0.4900 - val_loss: 1.3308 - val_acc: 0.5706\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.4436 - acc: 0.5210 - val_loss: 1.3081 - val_acc: 0.5746\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.3953 - acc: 0.5120 - val_loss: 1.2879 - val_acc: 0.5786\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.3921 - acc: 0.5120 - val_loss: 1.2689 - val_acc: 0.5798\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.3228 - acc: 0.5350 - val_loss: 1.2511 - val_acc: 0.5828\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.3101 - acc: 0.5390 - val_loss: 1.2342 - val_acc: 0.5846\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.2734 - acc: 0.5610 - val_loss: 1.2180 - val_acc: 0.5870\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.2827 - acc: 0.5510 - val_loss: 1.2026 - val_acc: 0.5913\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.2056 - acc: 0.5860 - val_loss: 1.1878 - val_acc: 0.5940\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.2376 - acc: 0.5820 - val_loss: 1.1743 - val_acc: 0.5975\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.1936 - acc: 0.5780 - val_loss: 1.1621 - val_acc: 0.6010\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.1751 - acc: 0.6050 - val_loss: 1.1507 - val_acc: 0.6036\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.1066 - acc: 0.6080 - val_loss: 1.1404 - val_acc: 0.6069\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.0734 - acc: 0.6300 - val_loss: 1.1311 - val_acc: 0.6098\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.1013 - acc: 0.6180 - val_loss: 1.1230 - val_acc: 0.6122\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.0775 - acc: 0.6400 - val_loss: 1.1169 - val_acc: 0.6128\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.0780 - acc: 0.6380 - val_loss: 1.1109 - val_acc: 0.6154\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 0s - loss: 1.0056 - acc: 0.6600 - val_loss: 1.1046 - val_acc: 0.6183\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.9763 - acc: 0.6660 - val_loss: 1.0971 - val_acc: 0.6200\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.9447 - acc: 0.6810 - val_loss: 1.0888 - val_acc: 0.6227\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.9640 - acc: 0.6660 - val_loss: 1.0811 - val_acc: 0.6241\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.9091 - acc: 0.7070 - val_loss: 1.0740 - val_acc: 0.6264\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.9057 - acc: 0.6880 - val_loss: 1.0684 - val_acc: 0.6290\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.9268 - acc: 0.6890 - val_loss: 1.0636 - val_acc: 0.6304\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.8614 - acc: 0.7090 - val_loss: 1.0595 - val_acc: 0.6327\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.8995 - acc: 0.6940 - val_loss: 1.0551 - val_acc: 0.6343\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.8226 - acc: 0.7260 - val_loss: 1.0515 - val_acc: 0.6361\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.7952 - acc: 0.7500 - val_loss: 1.0475 - val_acc: 0.6383\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.7825 - acc: 0.7320 - val_loss: 1.0443 - val_acc: 0.6389\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.7586 - acc: 0.7530 - val_loss: 1.0417 - val_acc: 0.6413\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.7355 - acc: 0.7650 - val_loss: 1.0398 - val_acc: 0.6424\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.7528 - acc: 0.7510 - val_loss: 1.0391 - val_acc: 0.6434\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.7321 - acc: 0.7480 - val_loss: 1.0392 - val_acc: 0.6443\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.7149 - acc: 0.7640 - val_loss: 1.0395 - val_acc: 0.6461\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6800 - acc: 0.7700 - val_loss: 1.0382 - val_acc: 0.6479\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6524 - acc: 0.7980 - val_loss: 1.0360 - val_acc: 0.6493\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6528 - acc: 0.7810 - val_loss: 1.0344 - val_acc: 0.6496\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.5956 - acc: 0.7980 - val_loss: 1.0334 - val_acc: 0.6505\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6087 - acc: 0.7980 - val_loss: 1.0335 - val_acc: 0.6514\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.6094 - acc: 0.8090 - val_loss: 1.0343 - val_acc: 0.6529\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.5871 - acc: 0.8110 - val_loss: 1.0359 - val_acc: 0.6536\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.5774 - acc: 0.8050 - val_loss: 1.0371 - val_acc: 0.6538\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.5684 - acc: 0.7910 - val_loss: 1.0366 - val_acc: 0.6552\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.5340 - acc: 0.8240 - val_loss: 1.0368 - val_acc: 0.6556\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.5026 - acc: 0.8290 - val_loss: 1.0387 - val_acc: 0.6554\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.4919 - acc: 0.8440 - val_loss: 1.0412 - val_acc: 0.6553\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.5061 - acc: 0.8350 - val_loss: 1.0430 - val_acc: 0.6562\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.4614 - acc: 0.8480 - val_loss: 1.0450 - val_acc: 0.6564\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.4515 - acc: 0.8550 - val_loss: 1.0463 - val_acc: 0.6569\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.5074 - acc: 0.8500 - val_loss: 1.0462 - val_acc: 0.6597\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.4172 - acc: 0.8660 - val_loss: 1.0482 - val_acc: 0.6615\n",
      "Epoch 83/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.4251 - acc: 0.8520 - val_loss: 1.0517 - val_acc: 0.6627\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3944 - acc: 0.8660 - val_loss: 1.0568 - val_acc: 0.6627\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.4105 - acc: 0.8700 - val_loss: 1.0622 - val_acc: 0.6627\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3978 - acc: 0.8640 - val_loss: 1.0678 - val_acc: 0.6630\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3721 - acc: 0.8840 - val_loss: 1.0728 - val_acc: 0.6628\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3893 - acc: 0.8670 - val_loss: 1.0774 - val_acc: 0.6636\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3279 - acc: 0.9070 - val_loss: 1.0819 - val_acc: 0.6641\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3257 - acc: 0.9010 - val_loss: 1.0871 - val_acc: 0.6643\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3425 - acc: 0.8990 - val_loss: 1.0933 - val_acc: 0.6635\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3104 - acc: 0.8890 - val_loss: 1.0985 - val_acc: 0.6645\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3074 - acc: 0.8980 - val_loss: 1.1030 - val_acc: 0.6661\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3074 - acc: 0.9000 - val_loss: 1.1072 - val_acc: 0.6666\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.3112 - acc: 0.8820 - val_loss: 1.1112 - val_acc: 0.6666\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.2773 - acc: 0.9190 - val_loss: 1.1158 - val_acc: 0.6664\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.2922 - acc: 0.9100 - val_loss: 1.1220 - val_acc: 0.6669\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.2721 - acc: 0.9060 - val_loss: 1.1278 - val_acc: 0.6668\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.2648 - acc: 0.9140 - val_loss: 1.1333 - val_acc: 0.6665\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 0s - loss: 0.2556 - acc: 0.9120 - val_loss: 1.1384 - val_acc: 0.6656\n",
      "Test loss: 1.13843721228\n",
      "Test accuracy: 0.6656\n"
     ]
    }
   ],
   "source": [
    "# TODO: define your model and hyperparams here\n",
    "# make sure to adjust the number of classes based on\n",
    "# the dataset\n",
    "# 10 for cifar10\n",
    "# 43 for traffic\n",
    "\n",
    "# TODO: train your model here\n",
    "\n",
    "batch_size = 512\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "learn_rate = 0.0001\n",
    "\n",
    "# input image dimensions\n",
    "# img_rows, img_cols = 32, 32\n",
    "# input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "## Bottleneck input shape\n",
    "input_shape = (1, 1, 2048)\n",
    "print('Bottleneck Shape',input_shape)\n",
    "\n",
    "\n",
    "## Train Model\n",
    "model = Sequential()\n",
    "# model.add(Dense(2048, activation='relu', input_shape=input_shape\n",
    "#                 ,kernel_initializer='TruncatedNormal'\n",
    "#                 ,kernel_regularizer=regularizers.l2(0.01)\n",
    "#                 ,activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(2048, activation='relu', input_shape=input_shape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=learn_rate),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1 ,\n",
    "          validation_data=(X_val, y_val))\n",
    "score = model.evaluate(X_val, y_val, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K.image_data_format()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
